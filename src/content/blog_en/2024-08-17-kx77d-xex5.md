---
title: "[Python] Using Generative AI to Summarize and Translate Foreign Tech Articles"
slug: kx77d-xex5
description: This article explains how to use Generative AI and LangChain to summarize foreign tech articles and translate them into Japanese, then automatically send them to tools like Discord. By using Generative AI, you can create desired features with minimal effort and few services. The process is introduced in Python.
date: 2024-08-17T07:40:20.728Z
preview: https://pub-21c8df4785a6478092d6eb23a55a5c42.r2.dev/img/eyecatch/langchain_eyecatch.webp
draft: false
tags: ['Python', 'LLM', 'LangChain']
categories: ['Programming']
---

# [Python] Using Generative AI to Summarize and Translate Foreign Tech Articles

I implemented a feature to summarize DevTo articles in Japanese and send them to Discord (repository: <https://github.com/rmc8/DevToDigest>).

## Overview

In [【Python】Getting Article Information from Dev Community (dev.to) via API](/introduction_devtopy), I created a module in Python to handle DevTo's API. Additionally, with the advent of GPT-4o-mini, individuals can now access high-performance models at a low cost via API, expanding the possibilities based on ideas (meanwhile, Gemini 1.5 Flash models under 128K are available at half the price of 4o-mini!). As it's said that there's no silver bullet in programming, having various techniques and flexible ideas is a weapon. Reading articles from overseas, which have a different culture, can help Japanese programmers develop unique perspectives. For me, even reading Japanese articles feels tedious, so I wrote this program to summarize English articles into Japanese for easy access to interesting ones.

## Code

The code mainly consists of three parts: (1) Article retrieval, (2) Article processing with Generative AI using LangChain, and (3) Data sending to Discord.

### Article Retrieval

Articles are retrieved using [devtopy](https://pypi.org/project/devtopy/).

```python
def fil_articles(
    articles: PublishedArticleList, threshold: int
) -> List[PublishedArticle]:
    return [a for a in articles.articles if a.positive_reactions_count > threshold]


res = dt.articles.get(page=1, per_page=1000, tag=tag_name.lower())
articles = fil_articles(res, reaction_threshold)
```

This retrieves up to 1000 articles being read that day and passes the data and threshold to the fil_articles function. The threshold is a filter based on the number of positive reactions, with a default value of 55, which roughly corresponds to the top 2% of reactions. This ensures the articles are sufficiently popular. By focusing on popular articles, you can reduce LLM costs and concentrate on high-quality content, providing an incentive to read even if English is challenging (for English learning * high-quality tech articles).

Afterward, since the retrieved and filtered article list doesn't include the full content, we fetch the article details. We use DevTo's API for this, but to reduce requests, we check via SQLite if the article has been processed by LLM before. This SQLite database is local, so it determines based on what the individual has read.

```python
for article in articles:
    url = str(article.url)
    if db.url_exists(url):
        continue
    article_id = article.id
    detail: Union[ErrorResponse, Article] = dt.articles.get_by_id(article_id)
    if type(detail) is ErrorResponse:
        continue
    title = article.title
    tags = article.tag_list
    contents = detail.body_markdown
```

If the URL has already been processed, it skips; otherwise, it requests detailed information using the article ID. If the article retrieval succeeds, it gets the title, tags, and article content, then proceeds to translation and summarization with LLM.

### Article Processing with Generative AI Using LangChain

The LangProcGpt class prepares for processing. This class handles requests to Generative AI and returns structured data.

```python
class LangProcGpt:
    MODEL_NAME = "gpt-4o-mini"

    def __init__(
        self,
        title: str,
        contents: str,
        url: str,
        img_url: str,
        tag_list: List[str],
        api_key: str,
    ):
        self.title = title
        self.contents = contents
        self.url = url
        self.tag_list = tag_list
        self.img_url = img_url
        self.llm = ChatOpenAI(
            model_name=self.MODEL_NAME, temperature=0, api_key=api_key
        )
        parser = PydanticOutputParser(pydantic_object=ProcResult)
        self.result_parser = OutputFixingParser.from_llm(
            parser=parser,
            llm=self.llm,
        )
```

The actual LLM processing is done on the title and contents, while the rest simply returns structured data or uses the API key for OpenAI authentication. Processing is executed via the run method.

```python
# main.py
params = {
            "title": title,
            "contents": contents,
            "url": url,
            "img_url": detail.social_image,
            "tag_list": tags,
            "api_key": os.getenv("OPENAI_API_KEY"),
        }
lp = LangProcGpt(**params)
data = lp.run(summary_sentences=summary_sentences)
```

```python
def run(self, summary_sentences: int = 5) -> ProcessedArticleData:
    result = self.summarize_and_translate(summary_sentences)
    res_dict = {
        "en_title": self.title,
        "ja_title": result.ja_title,
        "url": self.url,
        "img_url": str(self.img_url),
        "tags": ", ".join([f"#{t}" for t in self.tag_list]),
        "ja_summary": result.ja_summary,
    }
    return ProcessedArticleData(**res_dict)
```

In the run method, processing is consolidated in summarize_and_translate, and the final results are compiled into res_dict for structured data return. Let's check the prompts and processing one by one.

### Summarizing
```python
def summarize_and_translate(self, summary_sentences: int):
    summary = self.summarize(summary_sentences)  # Here
    ja_summary = self.translate(summary)
    ja_title = self.translate(self.title)
    # (Omitted)
```

Summarization is done in the summarize method, with an argument indicating the number of sentences for the summary. Since English text tends to be longer than Japanese for the same meaning, summarizing into N sentences rather than N characters is more appropriate for meaningful breaks.

```python
def summarize(self, summary_sentences: int):
    prompt = PromptTemplate(
        template=SUMMARIZE_PROMPT_TEMPLATE,
        input_variables=["summary_sentences", "contents"],
    )
    formatted_prompt = prompt.format(
        summary_sentences=summary_sentences, contents=self.contents
    )
    output = self.llm.invoke(formatted_prompt)
    return output.content
```

The summarize method uses a template to create the prompt.

```python
SUMMARIZE_PROMPT_TEMPLATE = """
## Instructions

Please summarize the following English text into a narrative of {summary_sentences} sentences in plain text format.

## Text

================================================================

{contents}

================================================================
""".strip()
```

The prompt instructs to summarize the English text into a narrative of N sentences in plain text format. This prevents simple bullet points and ensures a contextual, article-like summary. The text to summarize is passed in a Text block delimited by === to avoid conflicts with Markdown code blocks.

### Translating the Summary to Japanese

Next, the summarized text is passed to a translation task. While GPT-4o-mini is low-cost and high-performance, breaking tasks down simply improves accuracy. Thus, we first summarize in English and then translate the summary to Japanese. The prompt is as follows:

```python
JA_TRANSLATE_PROMPT_TEMPLATE = """
## Instructions
Please translate the following text into Japanese.

## Text
{text}
""".strip()
```

This is a simple prompt to translate the given text into Japanese. Since the previous summarization step instructs plain text output, the text is input directly. The article title is also translated using the same prompt.

### Structuring the Data

After completing the summarization and translation, we structure the data.

```python
class ProcResult(BaseModel):
    ja_title: str = Field(description="Japanese translation of the article title")
    ja_summary: str = Field(description="Japanese translation of the summary")

def summarize_and_translate(self, summary_sentences: int):
    summary = self.summarize(summary_sentences)
    ja_summary = self.translate(summary)
    ja_title = self.translate(self.title)
    formatted_instructions = self.result_parser.get_format_instructions()
    prompt = PromptTemplate(
        template=PARSE_PROMPT_TEMPLATE,
        input_variables=["ja_title", "ja_summary"],
        partial_variables={"formatted_instructions": formatted_instructions},
    )
    formatted_prompts = prompt.format(ja_title=ja_title, ja_summary=ja_summary)
    output = self.llm.invoke(formatted_prompts)
    return self.result_parser.parse(output.content)
```

formatted_instructions defines the structure of the desired data. It specifies returning a structured object with the Japanese title and summary, including descriptions for ja_title and ja_summary. By defining it this way, LangChain automates making it easier for the LLM to understand. After creating the description, we embed values into the prompt template.

```python
PARSE_PROMPT_TEMPLATE = """
## Instructions
Please return an object containing a Japanese title and a Japanese summary.

## Input
### Japanese Title
{ja_title}

### Japanese Summary
{ja_summary}

## Output Format
{formatted_instructions}
""".strip()
```

The prompt instructs to return a structured object with the Japanese title and summary. It passes the translated title and summary as input and the structured data definition from LangChain for output processing. The template creates the prompt, passes it to GPT-4o-mini, and extracts the data in JSON format from the response.

```python
prompt = PromptTemplate(
        template=PARSE_PROMPT_TEMPLATE,
        input_variables=["ja_title", "ja_summary"],
        partial_variables={"formatted_instructions": formatted_instructions},
    )  # Create the prompt template
formatted_prompts = prompt.format(ja_title=ja_title, ja_summary=ja_summary)  # Get the prompt with values embedded
output = self.llm.invoke(formatted_prompts)  # Pass the prompt to the model and get the response
return self.result_parser.parse(output.content)  # Extract structured data from the response based on the type definition
```

This completes the process, allowing the Japanese title and summary to be returned in a structured (programmable) format.

## Sending the Processed Results to Discord

We send the structured data to Discord. No need to create a bot; we can easily send messages using the Webhook feature. After sending, if there's no error, we record it in the database; otherwise, we move to the next article.

```python
lp = LangProcGpt(**params)
data = lp.run(summary_sentences=summary_sentences)
err_status = dw.report(data)
if err_status:
    continue
db.insert_data(
    url=data.url,
    en_title=data.en_title,
    ja_title=data.ja_title,
    translated_text=data.ja_summary,
    tags=data.tags,
)
```

Data sending to Discord doesn't use a dedicated library but can be done easily with requests.

```python
import requests
from .model import ProcessedArticleData


class DiscordWebhookClient:
    def __init__(self, webhook_url: str, is_silent: bool = True):
        self.webhook_url = webhook_url
        self.is_silent = is_silent

    def report(self, proc_data: ProcessedArticleData) -> int:
        embed = {
            "color": 0x00C0CE,
            "title": proc_data.ja_title,
            "url": proc_data.url,
            "fields": [
                {"name": "EnTitle", "value": proc_data.en_title},
                {"name": "Tags", "value": proc_data.tags},
                {"name": "Summary", "value": proc_data.ja_summary},
            ],
            "image": {"url": proc_data.img_url},
        }
        data = {
            "content": "",
            "embeds": [embed],
        }
        if self.is_silent:
            data["flags"] = 4096
        headers = {"Content-Type": "application/json"}
        response = requests.post(self.webhook_url, json=data, headers=headers)
        if response.status_code in (200, 204):
            return 0
        return response.status_code
```

Instantiate with the Webhook URL and silent notification setting, and pass the structured data to send it in a formatted state.

![report](https://pub-21c8df4785a6478092d6eb23a55a5c42.r2.dev/img/article/lc_devto_summary.webp)

It sends the Japanese title with a link, the original English title, tags, summary, and the article's eye-catching image, making it visually appealing and quick to decide whether to read the article in Japanese.

## Usage

You can send articles to Discord by running the command like this:

```shell
python main.py {tag name} {Webhook URL} {reaction threshold}
```

I have it set up on a Raspberry Pi 3 to run the following script automatically:

```shell
WEBHOOK_URL="https://discord.com/api/webhooks/xxxxxxxxxxx/yyyyyyyyyyyyyyzzzzzzzzzzzzz"
SCRIPT_PATH="$HOME/auto/DevToDigest/src/main.py"
THRESHOLD=100

cd $HOME/auto/DevToDigest/src
python3 $SCRIPT_PATH python $WEBHOOK_URL $THRESHOLD
python3 $SCRIPT_PATH typescript $WEBHOOK_URL $THRESHOLD
python3 $SCRIPT_PATH svelte $WEBHOOK_URL $THRESHOLD
python3 $SCRIPT_PATH flutter $WEBHOOK_URL $THRESHOLD
python3 $SCRIPT_PATH dart $WEBHOOK_URL $THRESHOLD
python3 $SCRIPT_PATH llm $WEBHOOK_URL $THRESHOLD
```

This aggregates and sends information related to topics I'm interested in, like Python, web, Flutter, and LLM.

## Thoughts

I've been using this since August 7, and it's helped me develop a habit of reading foreign tech articles, bookmarking interesting ones. There are best practices summarized in Japan, but also information not seen here, and the quality of popular articles is high. I charged only $5 and have used it for 10 days, with only $0.07 spent so far, making it very cost-effective. It's great for simple tasks, and with ideas, GPT-4o-mini can be useful for more complex ones too.

## Summary

Recent small-scale Generative AI models are low-cost and high-performance, offering individuals the potential to realize ideas that were previously difficult. Although not mentioned here, Function Calling, which distributes processing based on content, is now affordable. By identifying what Generative AI can do, composing prompts and overall processing, and executing ideas, you can create surprising apps. I hope this article serves as a simple starting point for such processing.

## Related Books

*Note: These are Amazon affiliate links.*
* [ChatGPT/LangChain Practical Introduction to Building Chat Systems](https://www.amazon.co.jp/dp/4297138395?&linkCode=ll1&tag=rmc-8-22&linkId=ab7c824b47e86760447c93a868d0ae0a&language=ja_JP&ref_=as_li_ss_tl): From the Engineer Selection series, it's reliable in quality. However, LangChain has evolved quickly, so the book's content is outdated; the latest is on GitHub. It's a good reference for processing assembly and prompt composition if you're okay with checking GitHub for updates.
* [Complete Introduction to LangChain: Mastering Large Language Models for Efficient Generative AI App Development](https://www.amazon.co.jp/dp/4295017965?&linkCode=ll1&tag=rmc-8-22&linkId=e6bda6f07cee8234ed2f39df6b4ad600&language=ja_JP&ref_=as_li_ss_tl): This focuses more on basics than the previous book, intensively covering prompt engineering and LangChain fundamentals. However, the code is displayed with line breaks, making the print version hard to read, and the content is outdated, so be cautious.
* [Textbook of Prompt Engineering for Mastering Large Language Models](https://www.amazon.co.jp/dp/4839985006?&linkCode=ll1&tag=rmc-8-22&linkId=7fb068c1178c91ce5fc9d917d7589a57&language=ja_JP&ref_=as_li_ss_tl): This is a pre-LangChain book touching on prompt engineering and ideas. It's like a prompt and idea collection, serving as a trigger to think about LangChain applications and useful for everyday Generative AI handling, so I recommend it.